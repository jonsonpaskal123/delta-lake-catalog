# نقشه راه پروژه: ETL از Elasticsearch به Delta Lake

## چشم‌انداز (Vision)

هدف اصلی این پروژه، ایجاد یک خط لوله (Pipeline) داده کارآمد و مقیاس‌پذیر برای انتقال داده‌ها از چندین ایندکس بزرگ در Elasticsearch به یک Data Lake مدرن مبتنی بر Delta Lake است. پس از انتقال، داده‌ها باید از طریق یک کاتالوگ متمرکز (Hive Metastore) قابل کشف و جستجو باشند تا بتوان تحلیل‌های پیچیده را روی آن‌ها اجرا کرد.

## معماری فعلی

در حال حاضر، زیرساخت اصلی پروژه با استفاده از Docker Compose راه‌اندازی شده و شامل سرویس‌های زیر است:

*   **Spark:** موتور پردازش داده.
*   **MinIO:** ذخیره‌ساز آبجکت (Object Storage) برای فایل‌های Delta Lake.
*   **Hive Metastore:** کاتالوگ متمرکز برای مدیریت متادیتای جداول.
*   **PostgreSQL:** پایگاه داده پشتیبان برای Hive Metastore.
*   **Elasticsearch:** منبع داده اولیه که شامل ایندکس‌های ما است.
*   **Kibana:** ابزار مشاهده و تحلیل داده‌های Elasticsearch.

## نقشه راه فازبندی شده

پروژه در سه فاز اصلی تعریف می‌شود:

### فاز ۱: پارامتریزه کردن و تعمیم اسکریپت ETL

در حال حاضر، اسکریپت `etl_elastic_to_delta.py` فقط برای یک ایندکس خاص (`person_sabt`) طراحی شده است. برای مدیریت چندین ایندکس، باید این اسکریپت را به یک ابزار عمومی تبدیل کنیم.

*   **اقدام:** اسکریپت `etl_elastic_to_delta.py` را طوری تغییر می‌دهیم که نام ایندکس منبع و نام جدول مقصد را به عنوان آرگومان‌های ورودی از خط فرمان (Command Line) دریافت کند.
*   **نتیجه:** یک اسکریپت واحد و قابل استفاده مجدد که می‌تواند هر ایندکسی را از Elasticsearch به یک جدول Delta در MinIO منتقل کند.

### فاز ۲: ایجاد یک اسکریپت ارکستراسیون (Orchestration)

پس از اینکه اسکریپت ETL ما عمومی شد، نیاز به راهی برای اجرای آن برای تمام ایندکس‌های مورد نظر داریم. اجرای دستی دستور برای هر ایندکس، کارآمد نیست.

*   **اقدام:** یک اسکریپت Shell (مثلاً `run_etl_pipeline.sh`) ایجاد می‌کنیم. این اسکریپت لیستی از نام ایندکس‌ها را می‌خواند و در یک حلقه (Loop)، اسکریپت ETL فاز ۱ را برای هر کدام از آن‌ها با آرگومان‌های مناسب اجرا می‌کند.
*   **نتیجه:** قابلیت اجرای کل فرآیند مهاجرت داده‌ها برای تمام ایندکس‌ها تنها با یک دستور.

### فاز ۳: پیاده‌سازی قابلیت جستجو و تحلیل

هدف نهایی، استفاده از داده‌های منتقل شده است. در این فاز، یک اسکریپت برای جستجو و تحلیل داده‌ها از طریق کاتالوگ ایجاد می‌کنیم.

*   **اقدام:** یک اسکریپت PySpark جدید (مثلاً `query_delta_lake.py`) می‌نویسیم. این اسکریپت:
    1.  یک یا چند جدول Delta را با استفاده از نام آن‌ها از کاتالوگ می‌خواند (مثلاً `spark.read.table("my_delta_db.person_sabt")`).
    2.  یک عملیات جستجو، فیلتر یا تجمیع (Aggregation) روی داده‌ها انجام می‌دهد.
    3.  نتایج را نمایش می‌دهد.
*   **نتیجه:** اثبات کارکرد کامل Pipeline و نمایش قدرت و سادگی دسترسی به داده‌ها از طریق کاتالوگ متمرکز.

---

با دنبال کردن این نقشه راه، ما به یک سیستم کامل و کارآمد برای مدیریت و تحلیل داده‌های خود خواهیم رسید.
