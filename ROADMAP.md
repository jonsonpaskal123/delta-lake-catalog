# نقشه راه پروژه: ETL از Elasticsearch به Delta Lake

## چشم‌انداز (Vision)

هدف اصلی این پروژه، ایجاد یک خط لوله (Pipeline) داده کارآمد و مقیاس‌پذیر برای انتقال داده‌ها از چندین ایندکس بزرگ در Elasticsearch به یک Data Lake مدرن مبتنی بر Delta Lake است. پس از انتقال، داده‌ها باید از طریق یک کاتالوگ متمرکز (Hive Metastore) قابل کشف و جستجو باشند تا بتوان تحلیل‌های پیچیده را روی آن‌ها اجرا کرد.

## معماری فعلی

در حال حاضر، زیرساخت اصلی پروژه با استفاده از Docker Compose راه‌اندازی شده و شامل سرویس‌های زیر است:

*   **Spark:** موتور پردازش داده.
*   **MinIO:** ذخیره‌ساز آبجکت (Object Storage) برای فایل‌های Delta Lake.
*   **Hive Metastore:** کاتالوگ متمرکز برای مدیریت متادیتای جداول.
*   **PostgreSQL:** پایگاه داده پشتیبان برای Hive Metastore.
*   **Elasticsearch:** منبع داده اولیه که شامل ایندکس‌های ما است.
*   **Kibana:** ابزار مشاهده و تحلیل داده‌های Elasticsearch.

## نقشه راه بازبینی شده (Revised Roadmap)

با توجه به پیچیدگی‌های احتمالی، نقشه راه پروژه را با رویکرد "اثبات کارکرد سپس تعمیم" (Prove then Generalize) بازبینی می‌کنیم. این رویکرد به ما کمک می‌کند تا ابتدا از صحت عملکرد کامل خط لوله برای یک نمونه مطمئن شویم و سپس آن را برای موارد بیشتر گسترش دهیم.

### فاز ۱: پیاده‌سازی و تست کامل خط لوله برای یک ایندکس (End-to-End Test for a Single Index)

در این فاز، تمام مراحل را فقط برای ایندکس `person_sabt` انجام می‌دهیم تا از کارکرد صحیح تمام اجزا مطمئن شویم.

*   **اقدام ۱.۱ (انجام شده):** پیاده‌سازی یک خط لوله ETL برای انتقال داده‌های ایندکس `person_sabt` از Elasticsearch به یک جدول Delta در MinIO و ثبت آن در Hive Metastore.
*   **اقدام ۱.۲ (فعلی):** پیاده‌سازی یک اسکریپت جستجو (`query_person_sabt.py`) که داده‌ها را مستقیماً از طریق **کاتالوگ** (`my_delta_db.person_sabt`) می‌خواند، یک فیلتر ساده روی آن اعمال می‌کند و نتیجه را نمایش می‌دهد. این کار صحت عملکرد کل خط لوله را از "نوشتن" تا "خواندن" تایید می‌کند.
*   **نتیجه:** اطمینان کامل از اینکه داده‌ها به درستی ذخیره، کاتالوگ و قابل بازیابی هستند.

### فاز ۲: پارامتریزه کردن و تعمیم اسکریپت‌ها

پس از اطمینان از عملکرد صحیح خط لوله، اسکریپت‌ها را برای استفاده عمومی آماده می‌کنیم.

*   **اقدام:** اسکریپت‌های `etl_elastic_to_delta.py` و `query_person_sabt.py` را طوری تغییر می‌دهیم که نام ایندکس و جدول را به عنوان آرگومان ورودی دریافت کنند.
*   **نتیجه:** دو اسکریپت قدرتمند و قابل استفاده مجدد برای ETL و جستجوی هر نوع داده‌ای.

### فاز ۳: ایجاد یک اسکریپت ارکستراسیون (Orchestration)

در این فاز، فرآیند اجرای خط لوله برای چندین ایندکس را خودکار می‌کنیم.

*   **اقدام:** یک اسکریپت Shell (مثلاً `run_etl_for_all.sh`) ایجاد می‌کنیم که لیستی از ایندکس‌ها را خوانده و اسکریپت ETL پارامتریزه شده را برای هر کدام اجرا می‌کند.
*   **نتیجه:** قابلیت اجرای کل فرآیند مهاجرت داده‌ها برای تمام ایندکس‌ها تنها با یک دستور.

---

این نقشه راه جدید، ریسک پروژه را کاهش داده و به ما اجازه می‌دهد تا به صورت گام به گام و با اطمینان بیشتری به سمت هدف نهایی حرکت کنیم.
