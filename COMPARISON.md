# مقایسه کاتالوگ Delta Lake در پروژه ما با StarRocks

این سند به مقایسه نحوه استفاده از کاتالوگ Delta Lake در پروژه فعلی ما (که مبتنی بر Apache Spark است) با مستندات ارائه شده برای StarRocks می‌پردازد.

## شباهت اصلی: نقش کاتالوگ

نکته کلیدی و شباهت اصلی در هر دو رویکرد، **نقش یکسان کاتالوگ** است. در هر دو سناریو، یک کاتالوگ خارجی (External Catalog) مانند **Hive Metastore** به عنوان یک "دفترچه تلفن" یا "فهرست مرکزی" برای متادیتا عمل می‌کند. این کاتالوگ وظیفه دارد به سوالات زیر پاسخ دهد:

*   چه دیتابیس‌هایی وجود دارند؟
*   هر دیتابیس شامل چه جداولی است؟
*   هر جدول در کدام مسیر فیزیکی (مثلاً در MinIO یا S3) ذخیره شده است؟
*   ساختار (Schema) هر جدول به چه صورت است؟

هم پروژه ما و هم StarRocks، به این کاتالوگ متصل می‌شوند تا بتوانند جداول Delta Lake را پیدا کرده و روی آن‌ها کوئری اجرا کنند.

## تفاوت اصلی: ابزار (موتور کوئری)

تفاوت بنیادین بین دو رویکرد، در **ابزاری** است که از این کاتالوگ برای پردازش داده‌ها استفاده می‌کند.

### ۱. پروژه ما: مبتنی بر Apache Spark

*   **موتور کوئری:** ما از **Apache Spark** استفاده می‌کنیم که ابزار استاندارد و نیتیو (Native) در اکوسیستم Big Data برای کار با فایل‌های Parquet و جداول Delta است.
*   **نحوه اتصال:** ما Spark را با استفاده از فایل‌های `hive-site.xml` و `spark-defaults.conf` پیکربندی کرده‌ایم تا به Hive Metastore متصل شود. این یک روش بسیار رایج و جاافتاده است.
*   **نحوه استفاده:** در کدهای PySpark، با دستور `spark.read.table("my_db.my_table")` به سادگی جداول را از طریق کاتالوگ می‌خوانیم.

### ۲. مستندات StarRocks: مبتنی بر StarRocks DB

*   **موتور کوئری:** در این سناریو، ابزار اصلی **StarRocks** است. StarRocks یک پایگاه داده تحلیلی (Analytical Database) مدرن و بسیار سریع است که می‌تواند به عنوان یک موتور کوئری قدرتمند روی منابع داده خارجی مانند Delta Lake عمل کند.
*   **نحوه اتصال:** به جای فایل‌های پیکربندی، در StarRocks با اجرای یک دستور SQL به نام `CREATE EXTERNAL CATALOG`، اتصال به Hive Metastore را تعریف می‌کنیم. این دستور تمام اطلاعات لازم (آدرس Hive Metastore، اطلاعات اتصال به MinIO و غیره) را در خود StarRocks ذخیره می‌کند.
*   **نحوه استفاده:** کاربران StarRocks با استفاده از کوئری‌های SQL استاندارد و با پیشوند نام کاتالوگ، به داده‌ها دسترسی پیدا می‌کنند: `SELECT * FROM my_catalog.my_db.my_table`.

## جدول خلاصه

| ویژگی | پروژه ما | StarRocks |
| :--- | :--- | :--- |
| **موتور کوئری** | Apache Spark | StarRocks |
| **کاتالوگ متادیتا** | Hive Metastore | Hive Metastore (یا AWS Glue) |
| **نحوه تعریف کاتالوگ**| از طریق فایل‌های `hive-site.xml` و `spark-defaults.conf` | با دستور `CREATE EXTERNAL CATALOG` در SQL |
| **نحوه خواندن داده** | `spark.read.table("...")` | `SELECT * FROM catalog.db.table` |

## نتیجه‌گیری

**بله، ما دقیقاً به همان صورت اصولی از کاتالوگ استفاده می‌کنیم.**

پروژه ما یک معماری کلاسیک و قدرتمند مبتنی بر Spark را پیاده‌سازی کرده است. StarRocks یک جایگزین مدرن برای بخش "موتور کوئری" است که می‌تواند در کنار یا به جای Spark قرار بگیرد. هر دو ابزار برای مدیریت متمرکز و کارآمد داده‌های Delta Lake، به یک کاتالوگ خارجی مانند Hive Metastore متکی هستند. بنابراین، مفهوم و نقش کاتالوگ در هر دو معماری کاملاً یکسان است.
