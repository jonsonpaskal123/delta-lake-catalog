# delta-lake-catalog

## کاتالوگ در Delta Lake چیست؟

کاتالوگ در Delta Lake به عنوان یک مخزن متادیتای مرکزی برای جداول Delta شما عمل می‌کند. به زبان ساده، این یک "فهرست" یا "راهنما" است که تمام اطلاعات مربوط به ساختار و سازماندهی داده‌های شما را در خود جای داده است. این قابلیت به موتورهای پردازشی مانند Apache Spark اجازه می‌دهد تا به طور کارآمد داده‌ها را پیدا کرده، بخوانند و پردازش کنند.

### وظایف اصلی کاتالوگ:

1.  **مدیریت متادیتا (Metadata Management):**
    *   **اسکیما (Schema):** ساختار جدول، نام ستون‌ها و نوع داده‌ی هر ستون را تعریف و نگهداری می‌کند.
    *   **پارتیشن‌بندی (Partitioning):** اطلاعات مربوط به نحوه پارتیشن‌بندی داده‌ها را ذخیره می‌کند تا کوئری‌ها بتوانند فقط پارتیشن‌های مورد نیاز را اسکن کنند و سرعت پردازش افزایش یابد.
    *   **مکان فیزیکی داده‌ها:** آدرس فایل‌های داده (معمولاً فایل‌های Parquet) که هر جدول را تشکیل می‌دهند، در کاتالوگ ثبت می‌شود.

2.  **گزارش تراکنش‌ها (Transaction Log):**
    *   هر تغییری که روی یک جدول Delta اعمال می‌شود (مانند `INSERT`, `UPDATE`, `DELETE`, `MERGE`) به عنوان یک "کامیت" در گزارش تراکنش‌ها (Transaction Log) ثبت می‌شود. این گزارش که به آن `_delta_log` نیز گفته می‌شود، قلب تپنده Delta Lake است.
    *   این گزارش به صورت یک سری فایل JSON ذخیره می‌شود و تضمین‌کننده ویژگی‌های **ACID** (Atomicity, Consistency, Isolation, Durability) برای جداول شماست.

3.  **سفر در زمان (Time Travel):**
    *   از آنجایی که تمام نسخه‌های متادیتا در گزارش تراکنش‌ها حفظ می‌شود، Delta Lake به شما این امکان را می‌دهد که به نسخه‌های قدیمی‌تر داده‌های خود دسترسی داشته باشید. این قابلیت که به آن "Time Travel" یا "Data Versioning" می‌گویند، برای موارد زیر بسیار مفید است:
        *   اشکال‌زدایی (Debugging) خطاها.
        *   بازگرداندن تغییرات ناخواسته.
        *   اجرای مجدد گزارش‌ها بر روی داده‌های یک تاریخ مشخص.

4.  **بهینه‌سازی کوئری (Query Optimization):**
    *   کاتالوگ آمارهای مربوط به داده‌ها (مانند مقادیر حداقل و حداکثر برای هر ستون) را جمع‌آوری و ذخیره می‌کند.
    *   موتورهای کوئری از این آمارها برای بهینه‌سازی پلن اجرای کوئری استفاده می‌کنند. برای مثال، اگر یک کوئری شرطی روی یک ستون داشته باشد که خارج از محدوده آماری یک فایل داده است، آن فایل به طور کامل از اسکن حذف می‌شود (Data Skipping).

### انواع کاتالوگ:

معمولاً دو نوع کاتالوگ در اکوسیستم Spark و Delta Lake استفاده می‌شود:

*   **کاتالوگ مبتنی بر فایل (File-based Catalog):** در این حالت، متادیتا مستقیماً در کنار داده‌ها در یک سیستم فایل (مانند HDFS یا S3) ذخیره می‌شود. این روش ساده است اما برای مدیریت متمرکز در محیط‌های بزرگ مناسب نیست.
*   **کاتالوگ خارجی (External Catalog):** مانند **Hive Metastore** یا **AWS Glue Data Catalog**. این کاتالوگ‌ها به عنوان یک سرویس متمرکز عمل می‌کنند و به چندین کلاستر و کاربر اجازه می‌دهند تا به طور همزمان به یک مجموعه از جداول دسترسی داشته باشند و متادیتای آن‌ها را به اشتراک بگذارند.

به طور خلاصه، کاتالوگ ستون فقرات مدیریت داده در Delta Lake است که ویژگی‌های قدرتمندی مانند تراکنش‌های ACID، سفر در زمان و عملکرد بالا را امکان‌پذیر می‌سازد.
