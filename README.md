# delta-lake-catalog

## کاتالوگ در Delta Lake چیست؟

کاتالوگ در Delta Lake به عنوان یک مخزن متادیتای مرکزی برای جداول Delta شما عمل می‌کند. به زبان ساده، این یک "فهرست" یا "راهنما" است که تمام اطلاعات مربوط به ساختار و سازماندهی داده‌های شما را در خود جای داده است. این قابلیت به موتورهای پردازشی مانند Apache Spark اجازه می‌دهد تا به طور کارآمد داده‌ها را پیدا کرده، بخوانند و پردازش کنند.

### وظایف اصلی کاتالوگ:

1.  **مدیریت متادیتا (Metadata Management):**
    *   **اسکیما (Schema):** ساختار جدول، نام ستون‌ها و نوع داده‌ی هر ستون را تعریف و نگهداری می‌کند.
    *   **پارتیشن‌بندی (Partitioning):** اطلاعات مربوط به نحوه پارتیشن‌بندی داده‌ها را ذخیره می‌کند تا کوئری‌ها بتوانند فقط پارتیشن‌های مورد نیاز را اسکن کنند و سرعت پردازش افزایش یابد.
    *   **مکان فیزیکی داده‌ها:** آدرس فایل‌های داده (معمولاً فایل‌های Parquet) که هر جدول را تشکیل می‌دهند، در کاتالوگ ثبت می‌شود.

2.  **گزارش تراکنش‌ها (Transaction Log):**
    *   هر تغییری که روی یک جدول Delta اعمال می‌شود (مانند `INSERT`, `UPDATE`, `DELETE`, `MERGE`) به عنوان یک "کامیت" در گزارش تراکنش‌ها (Transaction Log) ثبت می‌شود. این گزارش که به آن `_delta_log` نیز گفته می‌شود، قلب تپنده Delta Lake است.
    *   این گزارش به صورت یک سری فایل JSON ذخیره می‌شود و تضمین‌کننده ویژگی‌های **ACID** (Atomicity, Consistency, Isolation, Durability) برای جداول شماست.

3.  **سفر در زمان (Time Travel):**
    *   از آنجایی که تمام نسخه‌های متادیتا در گزارش تراکنش‌ها حفظ می‌شود، Delta Lake به شما این امکان را می‌دهد که به نسخه‌های قدیمی‌تر داده‌های خود دسترسی داشته باشید. این قابلیت که به آن "Time Travel" یا "Data Versioning" می‌گویند، برای موارد زیر بسیار مفید است:
        *   اشکال‌زدایی (Debugging) خطاها.
        *   بازگرداندن تغییرات ناخواسته.
        *   اجرای مجدد گزارش‌ها بر روی داده‌های یک تاریخ مشخص.

4.  **بهینه‌سازی کوئری (Query Optimization):**
    *   کاتالوگ آمارهای مربوط به داده‌ها (مانند مقادیر حداقل و حداکثر برای هر ستون) را جمع‌آوری و ذخیره می‌کند.
    *   موتورهای کوئری از این آمارها برای بهینه‌سازی پلن اجرای کوئری استفاده می‌کنند. برای مثال، اگر یک کوئری شرطی روی یک ستون داشته باشد که خارج از محدوده آماری یک فایل داده است، آن فایل به طور کامل از اسکن حذف می‌شود (Data Skipping).

### انواع کاتالوگ:

معمولاً دو نوع کاتالوگ در اکوسیستم Spark و Delta Lake استفاده می‌شود:

*   **کاتالوگ مبتنی بر فایل (File-based Catalog):** در این حالت، متادیتا مستقیماً در کنار داده‌ها در یک سیستم فایل (مانند HDFS یا S3) ذخیره می‌شود. این روش ساده است اما برای مدیریت متمرکز در محیط‌های بزرگ مناسب نیست.
*   **کاتالوگ خارجی (External Catalog):** مانند **Hive Metastore** یا **AWS Glue Data Catalog**. این کاتالوگ‌ها به عنوان یک سرویس متمرکز عمل می‌کنند و به چندین کلاستر و کاربر اجازه می‌دهند تا به طور همزمان به یک مجموعه از جداول دسترسی داشته باشند و متادیتای آن‌ها را به اشتراک بگذارند.

به طور خلاصه، کاتالوگ ستون فقرات مدیریت داده در Delta Lake است که ویژگی‌های قدرتمندی مانند تراکنش‌های ACID، سفر در زمان و عملکرد بالا را امکان‌پذیر می‌سازد.

### مثال کاربردی: سیستم فروش آنلاین

فرض کنید یک سیستم فروش آنلاین دارید که داده‌های مربوط به سفارشات (Orders) را در یک جدول Delta ذخیره می‌کنید. این جدول بر اساس ستون `order_date` پارتیشن‌بندی شده است.

1.  **ثبت سفارش جدید (`INSERT`):**
    *   وقتی یک مشتری سفارش جدیدی ثبت می‌کند، یک رکورد جدید به جدول `Orders` اضافه می‌شود.
    *   کاتالوگ متوجه این تغییر می‌شود و یک ورودی جدید در گزارش تراکنش‌ها (`_delta_log`) ایجاد می‌کند. این ورودی شامل اطلاعاتی مانند شناسه کامیت، زمان وقوع تراکنش و فایل داده جدیدی است که به پارتیشن مربوط به تاریخ امروز اضافه شده است.

2.  **لغو سفارش (`UPDATE`):**
    *   اگر مشتری سفارش خود را لغو کند، وضعیت سفارش در جدول `Orders` از "ثبت شده" به "لغو شده" تغییر می‌کند.
    *   Delta Lake به جای بازنویسی فایل داده قدیمی، یک فایل جدید با رکوردهای به‌روز شده ایجاد می‌کند و فایل قدیمی را برای "حذف منطقی" علامت‌گذاری می‌کند.
    *   کاتالوگ این عملیات `UPDATE` را نیز در گزارش تراکنش‌ها ثبت می‌کند و مشخص می‌کند که کدام فایل داده جدید جایگزین کدام فایل قدیمی شده است.

3.  **گزارش‌گیری ماهانه:**
    *   فرض کنید می‌خواهید گزارشی از تمام سفارشات ماه گذشته تهیه کنید. شما یک کوئری `SELECT` با شرط `WHERE order_date BETWEEN '2023-06-01' AND '2023-06-30'` اجرا می‌کنید.
    *   موتور Spark ابتدا به سراغ کاتالوگ می‌رود.
    *   کاتالوگ با استفاده از اطلاعات پارتیشن‌بندی، به سرعت تشخیص می‌دهد که فقط باید به سراغ پارتیشن‌های مربوط به ماه ژوئن برود و بقیه پارتیشن‌ها را نادیده می‌گیرد.
    *   علاوه بر این، با استفاده از آمارهای ذخیره شده (مانند حداقل و حداکثر تاریخ در هر فایل)، ممکن است حتی برخی از فایل‌های داخل پارتیشن‌های ماه ژوئن را نیز از اسکن حذف کند (Data Skipping).
    *   در نهایت، کاتالوگ آخرین نسخه معتبر فایل‌های داده را برای پردازش به Spark تحویل می‌دهد.

4.  **بازگرداندن یک حذف اشتباه (Time Travel):**
    *   تصور کنید یک کارمند به اشتباه تمام سفارشات یک روز خاص را حذف می‌کند.
    *   با استفاده از قابلیت Time Travel و گزارش تراکنش‌ها، شما می‌توانید به سادگی به نسخه‌ی جدول قبل از این حذف اشتباهی بازگردید. کاتالوگ به شما اجازه می‌دهد تا با یک کوئری ساده، وضعیت جدول را به یک زمان یا نسخه خاص برگردانید و داده‌های از دست رفته را بازیابی کنید.

در این سناریو، کاتالوگ مانند یک دفتر ثبت وقایع هوشمند عمل می‌کند که نه تنها تمام تغییرات را با دقت ثبت می‌کند، بلکه به سیستم کمک می‌کند تا داده‌ها را به بهینه‌ترین شکل ممکن پیدا و پردازش کند و از یکپارچگی آن‌ها محافظت نماید.
